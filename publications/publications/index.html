<header class="pub-heading">
	<h4>Manuscripts &amp; Preprints</h4>
</header>
<!-- Note that works listed below are subject to changes during the publication process. -->

<ol class="bibliography"><li>
  
    <abbr>[<a href="https://arxiv.org/" target="_blank">arXiv</a>]</abbr>
  


<div id="3">
  
    <span class="title">K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations</span>
    <span class="author">
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              <em>Cheulyoung Park</em>,
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                Narae Cha,
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://scholar.google.co.kr/citations?hl=en&amp;user=B9HMz0EAAAAJ" target="_blank">Soowon Kang</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://kimauk.github.io/" target="_blank">Auk Kim</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://scholar.google.com/citations?hl=en&amp;user=MpOViugAAAAJ" target="_blank">Ahsan Khandoker</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://scholar.google.com/citations?user=OfAkcXkAAAAJ&amp;hl=en" target="_blank">Leontios Hadjileontiadis</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://aliceoh9.github.io/" target="_blank">Alice Oh</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="http://ibrain.kaist.ac.kr/professor/" target="_blank">Yong Jeong</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
          <!-- for the last item in the list -->
            
              
                and <a href="http://ic.kaist.ac.kr/wiki/wiki.cgi?UichinLee" target="_blank">Uichin Lee</a> 
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>arXiv preprint arXiv:2005.04120</em>
    
    
      (to appear on Nature Sci. Data)
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
    [<a href="/assets/pdf/k-emocon.pdf" target="_blank">PDF</a>]
  
  
    [<a href="https://arxiv.org/abs/2005.04120" target="_blank">URL</a>]
  
  
  
  
  
    [<a href="https://doi.org/10.5281/zenodo.3931963" target="_blank">Dataset</a>]
  
  
    [<a href="https://github.com/Kaist-ICLab/K-EmoCon_SupplementaryCodes" target="_blank">Code</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Recognizing emotions during social interactions has many potential applications with the popularization of low-cost mobile sensors, but a challenge remains with the lack of naturalistic affective interaction data. Most existing emotion datasets do not support studying idiosyncratic emotions arising in the wild as they were collected in constrained environments. Therefore, studying emotions in the context of social interactions requires a novel dataset, and K-EmoCon is such a multimodal dataset with comprehensive annotations of continuous emotions during naturalistic conversations. The dataset contains multimodal measurements, including audiovisual recordings, EEG, and peripheral physiological signals, acquired with off-the-shelf devices from 16 sessions of approximately 10-minute long paired debates on a social issue. Distinct from previous datasets, it includes emotion annotations from all three available perspectives: self, debate partner, and external observers. Raters annotated emotional displays at intervals of every 5 seconds while viewing the debate footage, in terms of arousal-valence and 18 additional categorical emotions. The resulting K-EmoCon is the first publicly available emotion dataset accommodating the multiperspective assessment of emotions during social interactions.</p>
  </span>
  
</div>
</li></ol>

<header class="pub-heading">
	<h4>Refereed Conference Papers</h4>
</header>

<h3 class="year">2020</h3>
<ol class="bibliography"><li>
  
    <abbr>[<a href="https://dl.acm.org/journal/imwut" target="_blank">IMWUT</a>]</abbr>
  


<div id="2">
  
    <span class="title">"Hello There! Is Now a Good Time to Talk?": Opportune Moments for Proactive Interactions with Smart Speakers</span>
    <span class="author">
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                Narae Cha,
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://kimauk.github.io/" target="_blank">Auk Kim</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              <em>Cheulyoung Park</em>,
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://scholar.google.co.kr/citations?hl=en&amp;user=B9HMz0EAAAAJ" target="_blank">Soowon Kang</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                Mingyu Park,
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://dm.kaist.ac.kr/jaegil/" target="_blank">Jae-Gil Lee</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://scholar.google.com/citations?user=1cgtVW8AAAAJ&amp;hl=en" target="_blank">Sangsu Lee</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
          <!-- for the last item in the list -->
            
              
                and <a href="http://ic.kaist.ac.kr/wiki/wiki.cgi?UichinLee" target="_blank">Uichin Lee</a> 
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</em>
    
    
      (to appear)
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>We envision a wide range of novel proactive conversational services for smart speakers such as context-aware reminders and restocking household items. When initiating conversational interactions proactively, smart speakers need to consider users’ contexts to minimize disruption. In this work, we aim to broaden our understanding of opportune moments for proactive conversational interactions in domestic contexts. Toward this goal, we build a voice-based experience sampling device and conduct a one-week field study with 40 participants living in university dormitories. We collect 3,572 in-situ user experience reports and propose 19 activity categories to study contextual factors related to interruptibility. Our data analysis results show that the key determinants for opportune moments are closely related to both personal contextual factors such as busyness, mood, and resource conflicts for dual-tasking, and the other contextual factors associated with the everyday routines at home, including user mobility and social presence. Based on this finding, we discuss the need for designing context-aware proactive conversation management features that dynamically control conversational interactions based on users’ contexts and routines.</p>
  </span>
  
</div>
</li></ol>

<h3 class="year">2018</h3>
<ol class="bibliography"><li>
  
    <abbr>[<a href="https://dl.acm.org/conference/soups" target="_blank">SOUPS</a>]</abbr>
  


<div id="1">
  
    <span class="title">Share and Share Alike? An Exploration of Secure Behaviors in Romantic Relationships</span>
    <span class="author">
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              <em>Cheulyoung Park</em>,
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://corifaklaris.com/" target="_blank">Cori Faklaris</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://www.siyanz.com/" target="_blank">Siyan Zhao</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="http://www.alexsciuto.com/" target="_blank">Alex Sciuto</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="http://www.lauradabbish.com/" target="_blank">Laura Dabbish</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
          <!-- for the last item in the list -->
            
              
                and <a href="http://www.cs.cmu.edu/~jasonh/" target="_blank">Jason Hong</a> 
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Fourteenth Symposium on Usable Privacy and Security</em>
    
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">Abstract</a>]
  
  
  
  
    [<a href="/assets/pdf/soups2018-park.pdf" target="_blank">PDF</a>]
  
  
    [<a href="https://www.usenix.org/conference/soups2018/presentation/park" target="_blank">URL</a>]
  
  
  
  
  
  
  
    [<a href="https://www.cylab.cmu.edu/news/2018/09/07-romantic-couples-security.html" target="_blank">Press</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Security design choices often fail to take into account users’ social context. Our work is among the first to examine security behavior in romantic relationships. We surveyed 195 people on Amazon Mechanical Turk about their relationship status and account sharing behavior for a cross-section of popular websites and apps (e.g., Netflix, Amazon Prime). We examine differences in account sharing behavior at different stages in a relationship and for people in different age groups and income levels. We also present a taxonomy of sharing motivations and behaviors based on the iterative coding of open-ended responses. Based on this taxonomy, we present design recommendations to support end users in three relationship stages: when they start sharing access with romantic partners; when they are maintaining that sharing; and when they decide to stop. Our findings contribute to the field of usable privacy and security by enhancing our understanding of security and privacy behaviors and needs in intimate social relationships.</p>
  </span>
  
</div>
</li></ol>

